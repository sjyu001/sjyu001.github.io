<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sjyu001.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sjyu001.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-28T13:07:40+00:00</updated><id>https://sjyu001.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">gRNAde - Geometric Deep Learning for 3D RNA Inverse Design</title><link href="https://sjyu001.github.io/blog/2025/PAPER1/" rel="alternate" type="text/html" title="gRNAde - Geometric Deep Learning for 3D RNA Inverse Design"/><published>2025-05-27T00:00:00+00:00</published><updated>2025-05-27T00:00:00+00:00</updated><id>https://sjyu001.github.io/blog/2025/PAPER1</id><content type="html" xml:base="https://sjyu001.github.io/blog/2025/PAPER1/"><![CDATA[<h2 id="motivation-and-background">Motivation and Background</h2> <p>Designing RNA sequences that fold into a desired structure is a classic <strong>inverse folding problem</strong> in computational biology. Traditional methods often focus on matching a given secondary structure (2D base-pair pattern) and typically consider only a single target conformation. However, real RNA molecules are highly dynamic: a single sequence can adopt multiple distinct 3D conformations, which is crucial for functions like riboswitches and ribozymes<d-cite key="joshi2023multi"></d-cite>. Prior to gRNAde, the use of modern machine learning for RNA 3D design lagged behind protein design, due to limited RNA 3D data and the complexity of RNA flexibility. <strong>gRNAde</strong> (geometric <strong>RNA</strong> design) addresses this gap by explicitly accounting for RNA 3D structure and conformational diversity in sequence design. This aligns with recent trends in <strong>geometric deep learning</strong> that leverage graph neural networks (GNNs) for biomolecular structures, now extending those advances from proteins to the RNA world.</p> <p>The key motivation is to enable <strong>multi-state RNA design</strong>: instead of designing a sequence for just one rigid structure, design sequences that can stably fold into multiple specified shapes. This reflects real use-cases like riboswitches, which toggle between shapes, or RNAs that need to maintain structure under different conditions. Prior physics-based tools (like Rosetta) and learning models were limited to single states or 2D structure targets. gRNAde’s innovation is a learning-based pipeline that takes one or more RNA backbone conformations as input and produces nucleotide sequences that are predicted to fold into all those conformations. By incorporating 3D geometry and multiple conformers, gRNAde aims to improve design accuracy (recovering native sequences, achieving functional folds) and broaden the design space beyond static structures.</p> <hr/> <h2 id="methodology">Methodology</h2> <h3 id="multi-state-geometric-rna-design-pipeline">Multi-State Geometric RNA Design Pipeline</h3> <p><strong>gRNAde</strong> introduces a multi-stage model comprising a <strong>geometric multi-graph encoder</strong> and an <strong>autoregressive decoder</strong>. The overall goal is to estimate the probability of a sequence given the target RNA backbone structure(s), and then to sample sequences that maximize this probability. Formally, for a target RNA of length L with one or more backbone conformations, gRNAde models the conditional distribution and generates a sequence $x = (x_1, x_2, \dots, x_L)$ nucleotide by nucleotide as:</p> \[P(x_1, \dots, x_L \mid \text{backbone}(s)) = \prod_{i=1}^{L} P(x_i \mid \text{backbone}(s), x_1, \dots, x_{i-1}),\] <p>decoding from the 5’ end to 3’ end. This auto-regressive factorization allows the model to incorporate already-designed prefix bases when predicting the next base.</p> <h3 id="geometric-multi-graph-neural-network-encoder">Geometric Multi-Graph Neural Network Encoder</h3> <p>To <strong>featurize the 3D RNA backbones</strong>, gRNAde represents each RNA conformation as a <strong>geometric graph</strong>. Nodes correspond to nucleotides (located at the C4’ atom of the sugar), and edges capture local and spatial relationships between nucleotides. Notably, the authors use a 3-bead coarse-grained model: each nucleotide is represented by three key atoms (P, C4’, and the base’s N1/N9) to capture the backbone geometry in sufficient detail. Edges are formed between each nucleotide and its $k$ nearest neighbors in 3D (with $k=10$ in the implementation) based on C4’ distances. This builds a graph where spatially proximal nucleotides (which may interact or stack) are connected, beyond just the sequential neighbors. Rich geometric features are assigned to nodes and edges: for example, each node gets orientation vectors (pointing along the backbone direction and from sugar to base) akin to features used in protein GNNs. Edges similarly carry geometric features like the 3D displacement (as a unit vector), distance encoded by radial basis functions, and even the along-backbone distance encoded sinusoidally. This geometric featurization ensures the model can learn the relationship between sequence and 3D shape.</p> <p>Crucially, when <strong>multiple backbone conformations</strong> are provided, gRNAde does not simply merge them or choose one – instead, it constructs a <strong>“multi-graph” representation</strong> that preserves each conformation’s distinct information. If there are <em>M</em> conformations given, gRNAde builds M separate graphs (each with the same set of node indices, corresponding to the same nucleotide positions across conformers). These graphs are then stacked along a new axis to form a unified data structure for message passing. In practice, this means the node feature matrices from each conformation (size $L \times d$ for $L$ nodes and feature dimension $d$) are stacked into a tensor of shape $L \times M \times d$, and similarly for edge features. All graphs share the same node indexing (1 through <em>L</em> for the nucleotides), and an edge exists in the unified graph if it exists in any conformation’s graph (the union of edges). This clever construction allows a single GNN to <strong>process all conformations simultaneously</strong>: essentially, the model sees a tensor of node features where the second dimension corresponds to different conformers.</p> <p>The <strong>multi-state GNN encoder</strong> then performs message passing on this unified multi-graph. Intuitively, one can think of it as M parallel GNNs (one per conformation) that share weights and exchange information only by virtue of ultimately being merged in the output. The architecture is carefully designed to be <strong>equivariant and invariant to various symmetries</strong>: it is invariant to the order in which conformations are presented (shuffling the conformers shouldn’t change the encoding) and to rotations/translations of the 3D coordinates (a physical requirement), as well as the usual GNN invariance to node permutations. To achieve this, gRNAde’s message-passing layers treat the conformation axis as a set dimension. For example, the GNN uses SE(3)-equivariant operations (specifically a <strong>Graph Vector Perceptron</strong> or GVP GNN layer) that handle scalar and vector features, ensuring that rotating a backbone appropriately rotates those internal vector features while leaving scalars unchanged. During a message-passing update, a node’s features for each conformer are updated independently based on that conformer’s edges – the stacking ensures no accidental mixing of information between different conformations during the update step. This way, each conformation’s local structural context contributes to the node’s representation without “bleeding over” into other conformations, which guarantees the encoder respects the symmetry of swapping conformers.</p> <p>After several GNN layers (gRNAde uses 3 encoder layers in experiments), the model obtains M feature vectors for each nucleotide (one from each conformation’s branch). Now, to produce a single consolidated representation per nucleotide, the encoder applies a <strong>conformation order-invariant pooling</strong>: essentially a symmetric function (like elementwise average or sum) across the M conformer feature vectors. gRNAde uses a simple average pooling, which introduces no extra parameters. The result is a single enriched feature vector for each nucleotide position, encoding information from all the provided 3D shapes. Importantly, because of the invariance, providing the conformations in a different order would yield the same pooled result. This pooled set of node embeddings (length <em>*L</em>) now encapsulates the structural constraints of potentially multiple backbone states.</p> <hr/> <h2 id="autoregressive-sequence-decoder">Autoregressive Sequence Decoder</h2> <p>Given the pooled node representations from the encoder, gRNAde employs an <strong>autoregressive decoder</strong> to construct the RNA sequence one nucleotide at a time. The decoder is also implemented as GVP-based GNN layers operating on the nucleotide graph, but now the task is to output a probability distribution over the four bases {A, U, G, C} for each position in sequence order. The decoding proceeds iteratively from the 5’ end (position 1) to the 3’ end (position <em>L</em>): at each step <em>i</em>, the decoder is conditioned on the previously decided nucleotides $x_1,\dots,x_{i-1}$ (for <em>i</em> = 1, no prior context) and the encoder’s structural embeddings, and predicts the probability of each nucleotide at position <em>i</em>. In practice, this is done by masking unknown future positions and iteratively unmasking them in sequence order. The decoder uses 3 GNN layers as well (mirroring the encoder) and at each position incorporates the already-decoded neighbors as features (often via one-hot encoding of known bases in the node feature). This approach is analogous to how autoregressive protein design models (like Ingraham et al. 2019 or ProteinMPNN) operate on graphs.</p> <p>One advantage of this decoder design is flexibility: while the default is 5’→3’ sequential decoding, the authors note that <strong>unordered or masked decoding</strong> schemes could also be used (similar to non-sequential strategies in protein design). For example, one could design all nucleotides in parallel or in random order, or even fill in specific parts (if some nucleotides are pre-defined). In their experiments, however, sequential decoding was effective and straightforward.</p> <hr/> <h2 id="experimental-results-and-key-contributions">Experimental Results and Key Contributions</h2> <p>gRNAde was evaluated on both standard benchmarks and new tasks, showing strong performance improvements over prior methods. The contributions and findings of the paper can be summarized as follows:</p> <ul> <li> <p><strong>State-of-the-Art Accuracy on Fixed-Backbone RNA Design</strong>: On a benchmark of 14 diverse RNA structures (originally posed by Das et al. 2010), gRNAde achieved significantly higher <strong>native sequence recovery</strong> (the percentage of positions where the designed sequence matches the natural RNA sequence that folds into that structure) compared to Rosetta’s RNA design protocol. gRNAde recovered about <strong>56%</strong> of native nucleotides on average, outperforming Rosetta’s <strong>45%</strong> average. Not only is it more accurate, but it’s remarkably <strong>faster</strong> – gRNAde can generate hundreds of candidate sequences in under a second (using a GPU) for an RNA of ~60 nucleotides, whereas Rosetta takes hours for a single design on CPU. This demonstrates the efficiency of the learned GNN approach over physics-based sampling. It’s important to note that Rosetta (as of Leman et al. 2020) was considered a state-of-the-art physics-driven method for RNA 3D design, so gRNAde establishing a new high in both accuracy and speed is a significant milestone.</p> </li> <li> <p><strong>Enabling Multi-State RNA Design</strong>: gRNAde is the first approach to handle multiple target conformations in RNA design. The authors created a <strong>new multi-state RNA design</strong> benchmark with structurally flexible RNAs (cases where an RNA has two or more known distinct structures). In this setting, they found that a multi-state version of gRNAde (leveraging the multi-graph encoder) could design sequences that better satisfy all conformations simultaneously, compared to a single-state model that only sees one shape at a time. Specifically, multi-state gRNAde improved sequence recovery by about <strong>5%</strong> on average over the single-structure baseline for these flexible RNAs. Qualitatively, the improvements were most pronounced at nucleotide positions that undergo large positional or base-pairing changes between conformations (e.g. loop regions that shift). This highlights that incorporating conformational diversity in the model leads to more robust sequences that can tolerate and maintain multiple shapes – something not possible with previous design methods (indeed, Rosetta cannot inherently do multi-state design for RNAs).</p> </li> <li> <p><strong>Zero-Shot Generalization to Mutational Fitness Landscapes</strong>: Beyond designing entirely new sequences, gRNAde can be used to evaluate sequences for a given structure. The paper demonstrates a <strong>zero-shot fitness ranking</strong> experiment on an RNA enzyme (ribozyme) mutation dataset. By computing the model’s perplexity (essentially, how well the model predicts a given sequence) for variants of the ribozyme, they showed that lower perplexity correlates with higher functional fitness. In other words, gRNAde’s learned distribution assigns higher likelihood to functional or native-like sequences. This allowed the authors to rank mutations without any additional training (“zero-shot”), outperforming a random guessing baseline in identifying beneficial mutations. Such capability is important for guiding experimentalists in narrowing down candidate sequences in directed evolution or RNA design cycles</p> </li> <li> <p><strong>Experimental Validation via Eterna OpenKnot Challenge</strong>: Perhaps most compelling, gRNAde was tested in the wet lab. The authors participated in Eterna’s OpenKnot challenge, designing 200 RNA sequences for 10 different RNA backbone targets (each containing complex pseudoknots, which are difficult RNA tertiary motifs). The designs were synthesized and experimentally probed using SHAPE chemical mapping to see if they fold into the desired structures. gRNAde achieved a <strong>50% success rate</strong> in these trials, meaning half of the tested sequences folded correctly into the target pseudoknotted structures. This is a substantial improvement over Rosetta’s success rate (~35%) under the same conditions. This result is significant because pseudoknotted RNAs are notoriously challenging to design (many prior computational tools avoid pseudoknots entirely). A 50% success rate in lab tests suggests gRNAde is learning meaningful rules of RNA folding and can produce experimentally viable designs. It marks one of the first times a learning-based RNA design model has been validated at this scale in the lab.</p> </li> </ul> <p>In addition to the above, gRNAde introduced a <strong>new RNA design dataset</strong> derived from the RNAsolo database, comprising 3,764 unique RNA sequences each with 3D structure(s) (over 11,000 total structures). The dataset was split in multiple challenging ways (by conformational diversity, by number of conformers, by sequence dissimilarity) to rigorously evaluate generalization. All these contributions together demonstrate gRNAde as a comprehensive platform for RNA design, pushing the frontier of what is achievable with geometric deep learning on RNA.</p> <hr/> <h2 id="limitations-and-future-directions">Limitations and Future Directions</h2> <p>While gRNAde represents a significant advance, there are some limitations and assumptions in its current form. First, gRNAde treats the RNA backbone structure(s) as given inputs – it does <em>fixed-backbone</em> design only. This means it cannot suggest new 3D shapes on its own; it needs an initial model or experimentally determined structure to design against. In practice, this is often fine (one might have a target shape in mind from experiment or imagination), but future work could integrate <strong>structure prediction</strong> and design, allowing suggestions of novel shapes alongside sequences. The authors themselves hint that the 3-bead graph representation could be used for RNA backbone generation in the future, which would move toward de <em>novo</em> RNA 3D design.</p> <p>Another limitation is data scarcity and bias: RNA 3D structures in the PDB are relatively few and often biased toward certain motifs (e.g., many tRNAs or riboswitches). gRNAde’s training data, though larger than past works, is still thousands of sequences – orders of magnitude smaller than protein structure datasets. The model might struggle with very large RNAs (there were only a handful of &gt;1000-nt examples) or with RNA topologies that were absent in training. Moreover, gRNAde’s performance drops if asked to design for a structure far outside its training distribution (though the paper showed decent generalization via sequence identity splitting). As more RNA structures become available (e.g., via experimental methods or computational predictions), gRNAde could be retrained or scaled up, which would likely improve its coverage of RNA structural space.</p> <p>The multi-state design ability is novel, but comes with assumptions: it assumes the set of input conformations is representative of the important states of the RNA. If the RNA has other unseen states or intermediate folding forms, the designed sequence might still fail. There’s also an implicit assumption that the sequence should stabilize all provided conformations, which might trade off stability in each individual state. In practice, designing for multiple states can be at odds with optimizing one state fully – gRNAde addresses this with an ML approach, but it might sometimes output a compromise sequence that is not optimal for any single state. Future work might explore weighting different states or incorporating kinetic/folding pathway information, so that certain conformations are prioritized or the transition barriers are considered.</p> <p>From a model perspective, gRNAde currently uses relatively shallow GNN layers (3 encoder + 3 decoder) and an average pooling. It’s possible that deeper networks or more sophisticated pooling (learned pooling or attention across conformers) could capture interactions between conformations better. Additionally, the decoder could potentially be improved by non-sequential strategies or incorporating global sequence constraints (like enforcing a certain base composition or motif presence). The authors note that techniques from protein design like masked token decoding could be applied – this could allow partial sequence specification or inpainting (filling in missing pieces of an RNA given the rest), which would increase gRNAde’s utility in practical design scenarios.</p> <p>Lastly, while gRNAde showed promising lab validation, a 50% success rate means there is still room to grow. The failures might be due to subtle effects not captured by the model (ion interactions, non-canonical base pairs, tertiary contacts beyond 10Å that were not encoded, etc.). Integrating some physics-informed features or energy terms (perhaps via an energy-based reranker or incorporating known thermodynamic parameters as features) might further improve the success for difficult targets. Nonetheless, gRNAde provides a powerful foundation on which such future improvements can be built.</p> <hr/> <h2 id="conclusion-and-impact">Conclusion and Impact</h2> <p>gRNAde exemplifies how geometric deep learning can push forward the field of RNA design, analogous to how protein design has benefited from GNN and transformer models. By leveraging 3D information and embracing RNA’s intrinsic flexibility, it achieves state-of-the-art results and introduces capabilities (like multi-state design and rapid scoring of variants) that were previously out of reach for RNA engineers. This work sits at the intersection of structural biology and machine learning, and it underscores a broader trend: ML models are now sophisticated enough to handle the complexities of RNA structure (including pseudoknots and alternative folds) which used to be exclusively the domain of physics-based models.</p> <p>From a practical standpoint, gRNAde could be a game-changer for researchers designing RNA-based therapeutics and nanotechnology. For example, one could envision designing a riboswitch that must adopt two distinct conformations – gRNAde provides a tool to tackle that directly. It also offers a new paradigm for multi-objective design (designing sequences that meet multiple structural criteria). The methods introduced, such as the multi-graph GNN encoder, could be generalized to other problems where inputs are an ensemble of related structures (not just RNA – perhaps protein loops in different conformations, or small molecules in multiple poses). In summary, gRNAde’s comprehensive approach to RNA inverse folding demonstrates the power of modern ML in solving long-standing biological design problems, and it opens the door for more dynamic, ensemble-aware design techniques in the future.</p>]]></content><author><name>Seungjun Yu</name></author><category term="Paper"/><category term="Review"/><category term="1"/><summary type="html"><![CDATA[Chaitanya K. Joshi, Arian R. Jamasb, Ramon Viñas, Charles Harris, Simon V. Mathis, Alex Morehead, Rishabh Anand, Pietro Liò (ICLR 2025)]]></summary></entry><entry><title type="html">ProtComposer - Compositional Protein Structure Generation with 3D Ellipsoids</title><link href="https://sjyu001.github.io/blog/2025/PAPER2/" rel="alternate" type="text/html" title="ProtComposer - Compositional Protein Structure Generation with 3D Ellipsoids"/><published>2025-05-27T00:00:00+00:00</published><updated>2025-05-27T00:00:00+00:00</updated><id>https://sjyu001.github.io/blog/2025/PAPER2</id><content type="html" xml:base="https://sjyu001.github.io/blog/2025/PAPER2/"><![CDATA[<h2 id="motivation-and-idea">Motivation and Idea</h2> <p>Modern generative models have shown impressive ability to <strong>hallucinate new protein structures</strong> (novel folds or designs not seen in nature). However, a major limitation has been the lack of control over the generated protein’s high-level structure or function. In image generation, users can guide generation through sketches, bounding boxes, or text prompts – by contrast, protein designers until now had little ability to specify where a model should place certain structural elements, or the overall shape of the protein. <strong>ProtComposer</strong> addresses this by introducing a <strong>compositional generation</strong> approach for proteins, where the user (or an algorithm) provides a rough <strong>3D layout</strong> for the protein, and the model then fills in a protein structure that matches this outline. The layout is specified in terms of simple geometric primitives: <strong>3D ellipsoids</strong> that encode the position, orientation, size, and even intended secondary structure of protein substructures. In essence, ProtComposer lets one <strong>sketch a protein in 3D</strong> using ellipsoidal blobs – each blob might represent, say, “an alpha-helix of ~10 residues oriented this way” or “a beta-sheet domain roughly here” – and then generates a full atomic protein backbone and sequence that realizes that sketch.</p> <p>This innovation is significant in the context of protein design. Natural proteins are often <strong>modular</strong>, consisting of distinct domains or motifs that come together to form larger complexes. Designing such multi-domain proteins or enforcing specific spatial arrangements (for example, placing two functional domains at a certain distance) is very challenging for unconditional models – they might produce something plausible, but not necessarily the desired arrangement. ProtComposer’s ellipsoid-based conditioning provides a new level of <strong>controllability</strong>: one can dictate high-level features like <em>overall shape (globular vs elongated), composition of secondary structures (helix vs sheet content), and relative domain positioning</em>. This approach parallels ideas from other domains (the authors compare it to “bounding box” or “blob” based conditioning in image generation), transferring those concepts to protein structures.</p> <p>The motivation also stems from an observation: prior protein generative models tended to produce relatively simple and often repetitive structures, such as <strong>helix bundles</strong> (lots of alpha-helices packed together). These are common because they are easy for models to learn and stable to form, but they are <em>“conceptually simple”</em> and less diverse than what nature shows. By forcing the model to condition on a variety of ellipsoid layouts – especially ones that include beta-sheets or mixed structure types – ProtComposer aims to break the bias towards overly helical proteins and expand the diversity of generated folds. Indeed, one of the paper’s key results is that by using random synthetic ellipsoid layouts as input, they obtain proteins with a <strong>helix fraction matching that of natural proteins</strong>, whereas earlier methods oversampled helices.</p> <p>In summary, ProtComposer is motivated by the need for <strong>controllable and diverse protein structure generation</strong>. It provides a way to <strong>specify design intent</strong> (through coarse 3D descriptors) and then uses a generative model to produce a detailed protein backbone and amino acid sequence that fits that intent.</p> <hr/> <h2 id="method-and-architecture">Method and Architecture</h2> <p>At its core, ProtComposer builds on a state-of-the-art generative model for proteins known as <strong>Multiflow</strong>, which is a joint sequence–structure continuous flow model (developed by some of the same authors) that can generate protein backbones and sequences simultaneously. Multiflow itself is a type of <strong>SE(3)-equivariant flow-matching model</strong> for proteins – it represents a protein as a set of residue “frames” in 3D (each residue has a position and orientation, and an amino acid type) and learns to gradually transform a simple initial distribution (random noise) into the distribution of real protein structures and sequences. It uses techniques like Invariant Point Attention (from AlphaFold) to handle 3D geometry and includes components to generate the discrete sequence along with continuous coordinates.</p> <p>ProtComposer augments this generative framework with <strong>ellipsoid-based conditioning</strong>. An ellipsoid for the model is defined by parameters capturing its center (3D coordinates), its orientation (which can be encoded by principal axes or a rotation matrix), its radii (defining the ellipsoid shape – slender for a rod-like helix, flat for a sheet, etc.), and annotations like the number of residues it should contain and the dominant secondary structure type (α-helix, β-sheet, or coil). These ellipsoids are essentially high-level placeholders for chunks of protein structure. For example, a long thin ellipsoid annotated as “helix, 20 residues” suggests the model should place an alpha-helix of about 20 amino acids running roughly along that ellipsoid’s length. Multiple ellipsoids together might describe a desired topology: e.g., two ellipsoids side by side (one helical, one sheet-like) could indicate a two-domain protein with one helical domain and one β-sheet domain.</p> <p><strong>Incorporating ellipsoids into the generative model</strong> is done via a mechanism the authors call <strong>Invariant Cross Attention (ICA)</strong>. In practice, they introduce a set of <strong>ellipsoid tokens</strong> alongside the protein’s own residue tokens in the model’s architecture. During generation, the model’s layers perform cross-attention between the evolving protein representation and the fixed ellipsoid representations in an SE(3)-invariant way. This means each residue can attend to the ellipsoids, taking into account the relative 3D positions between a residue and an ellipsoid (similar to how AlphaFold’s attention uses relative positional encoding of residues). The invariance ensures that if you rotate or translate the entire setup, the relationship doesn’t change – only relative geometry matters. Essentially, the model learns to “pull” the growing protein toward the ellipsoids: residues will organize and align to fill those ellipsoids because the attention provides a directional signal. The authors fine-tuned the pretrained Multiflow model with this new cross-attention mechanism, rather than training from scratch, which they note is akin to how image models are fine-tuned for conditioning with minimal perturbation of the original model’s behavior. If no ellipsoid is provided, the model should revert to the unconditional generator (thus they take care to ensure an empty ellipsoid set leaves the model unchanged, via how they initialize the conditional training).</p> <p>Under the hood, the generative process is based on <strong>flow matching</strong>, which is an approach to generative modeling where one learns a continuous vector field that morphs a distribution of noise into the data distribution. ProtComposer’s flow operates over three coupled spaces: the 3D coordinates of residues, the orientations (rotations) of residue frames, and the discrete amino acid identities. The paper describes using separate flow components for handling <strong>translations, rotations</strong>, and <strong>discrete sequence</strong> respectively, which are iteratively applied. For example, they use a linear flow on $\mathbb{R}^3$ for coordinate shifts, a Riemannian flow on SO(3) for orientations, and a discrete flow matching method for the amino acid types. All these are conditioned on the ellipsoids via the invariant cross attention in the network that computes the instantaneous vector field. The training objective ensures that the presence of ellipsoid conditioning steers the flow to produce structures consistent with those ellipsoids.</p> <p>To sample a new protein given a set of ellipsoids, ProtComposer starts from random noise (e.g., a random set of points and random sequence) and then integrates the <strong>learned vector field</strong> (essentially denoising) until a structured protein is obtained. The process can be guided with a strength parameter – they implement a form of <strong>classifier-free guidance</strong> that allows interpolating between unconditional generation and fully-conditioned generation. With guidance, one can trade off how strictly the model adheres to the ellipsoid layout versus how much it improvises. In experiments, they varied a guidance factor $\lambda$ to adjust this trade-off, observing that higher conditioning strength yields better alignment with ellipsoids at some cost to sample diversity (typical of guidance).</p> <hr/> <h2 id="capabilities-and-experimental-results">Capabilities and Experimental Results</h2> <p>ProtComposer was evaluated on a variety of tasks to demonstrate: (1) its ability to <strong>faithfully adhere</strong> to specified layouts (control accuracy), (2) the increase in <strong>diversity and novelty</strong> of generated structures when using novel layouts, and (3) the generation of structurally <strong>complex compositions</strong> that prior models struggle with. Key findings include:</p> <ul> <li> <p><strong>Strong Adherence to Layout Constraints</strong>: The authors define metrics to quantify how well the generated protein matches the input ellipsoids – for instance, how closely each ellipsoid’s position and shape are filled by the corresponding protein region, and whether the secondary structure content (helix vs sheet) in that region matches the annotation. ProtComposer achieves high consistency on these metrics. Even for layouts that are quite different from anything in the training data (e.g., novel arrangements of helices and sheets), the model can realize them with remarkable fidelity. In some examples, they show that if you specify alternating helix and sheet ellipsoids in a ring, the model will produce a protein that indeed has alternating helix and sheet segments arranged in a ring-like fashion – something an unconditional model would rarely do. Importantly, this consistency holds beyond the training distribution: they test some wild, hand-drawn ellipsoid configurations and find the model still places structured protein elements accordingly. This demonstrates that ProtComposer’s conditioning is robust and not simply memorizing training pairs.</p> </li> <li> <p><strong>Improved Diversity and Novelty of Generated Proteins</strong>: By sampling <strong>random ellipsoid</strong> layouts from a simple generative process (the authors built a heuristic model that creates random sets of ellipsoids with random sizes/positions, to serve as a source of many plausible but novel protein “outlines”), they can drive the generator to explore more of the protein structure space. The results show a significant increase in both <strong>novelty</strong> (measured by how different the generated folds are from any known protein, using metrics like TM-score or fragment frequencies) and <strong>diversity</strong> (variability among generated samples) when using these synthetic layouts. Essentially, the model is no longer confined to producing the few common motifs it might favor unconditionally; the layouts serve to pull it into new territory. There is a slight trade-off: some extremely novel or complex layouts may yield proteins that are a bit <strong>less designable</strong>, where designability refers to how likely the sequence is to actually fold into the structure (they often assess this by seeing if AlphaFold or similar can confidently recapitulate the structure from the sequence). The paper notes a cost to designability when pushing for more diversity. However, they illustrate that by tuning the guidance strength, one can navigate a <strong>Pareto frontier</strong> between novelty/diversity and designability. In other words, ProtComposer allows the user to decide how adventurous to be: with strong conditioning, you get very novel shapes at some risk of lower foldability; with weaker conditioning, you stay closer to known protein space but with higher confidence in realistic folding. This tunable balance is a valuable feature for practical design – early exploratory phases might prioritize novelty, while later stages tune for higher designability.</p> </li> <li> <p><strong>Generation of Compositional and Complex Structures</strong>: ProtComposer demonstrates the ability to generate proteins with <strong>multiple distinct substructures</strong> (e.g., a protein with a helical bundle on one end and a beta-sheet sandwich on the other, connected by a loop) – essentially <em>chimeric</em> architectures that combine different fold motifs. Previous generative models often default to a single motif repeated (like several helices or a simple up-down-beta sheet). The paper argues that high-desirability proteins often exhibit such modular complexity – akin to having “different spatial parts with different properties” working together – but models without spatial control rarely produce them because it’s a low-probability event to spontaneously form two distinct domains. By explicitly conditioning on a layout that encodes different parts, ProtComposer can routinely generate these <strong>multi-domain compositions</strong>. They even introduce a metric for “compositionality” which checks if a generated protein contains a mix of secondary structure elements in separate spatial regions (rather than all helices or all strands clumped). ProtComposer improves this compositionality metric, indicating it can make proteins that are not just novel in shape but also <strong>rich in structural heterogeneity</strong>, better mimicking natural proteins which often have complex domain organizations. In one example, the authors mention how an unconditional model might make a protein that is essentially a single large alpha-helix bundle (low compositional complexity), whereas ProtComposer could create one that has an $\alpha$-$\beta$ mixed domain connected to an all-$\alpha$ domain, etc., thereby achieving higher functional potential.</p> </li> <li> <p><strong>Editing and Re-mixing Existing Proteins</strong>: Another neat capability is using ellipsoids extracted from real proteins. One can take a known protein, abstract its shape into a few ellipsoids (perhaps one per domain), and then provide those to ProtComposer. The model can generate new protein structures that fit the same overall layout but with different connectivity or features. For instance, if a known protein has two domains connected in a particular orientation, ProtComposer can redesign the interface or the structure of each domain while keeping the overall arrangement. This is useful for <strong>protein redesign</strong> and <strong>scaffold hopping</strong> – you maintain the high-level architecture (maybe necessary for function) but explore novel ways to implement it. The paper demonstrates cases where the connectivity between substructures is altered (like loops re-routed) yet the ellipsoid placement is satisfied. Essentially, ProtComposer can recompose proteins, which is an interesting approach to invent variants of existing machines or create chimeras in a controlled way.</p> </li> </ul> <p>Given these results, ProtComposer was recognized as achieving state-of-the-art performance in the realm of controllable protein generation – it was an <strong>ICLR 2025 Oral paper</strong> (top ~2% of submissions). Its ability to span the space between trained data and novel configurations, while maintaining realism, sets a new benchmark. The code was made available, hinting that the community can start using this tool for real design tasks.</p> <hr/> <h2 id="limitations-and-future-direction">Limitations and Future Direction</h2> <p>Despite its powerful capabilities, ProtComposer has several limitations and assumptions to be aware of:</p> <ul> <li> <p><strong>Reliance on Input Specification</strong>: The quality of generated proteins is tied to the quality of the ellipsoid layout provided. Designing a good set of ellipsoids for a desired function can be non-trivial. In practice, a user might not know exactly what ellipsoid arrangement yields, say, a binding pocket or an enzyme active site – more intuitive or higher-level control (like “make a pocket here”) is still an open challenge. The ellipsoid abstraction, while flexible, assumes the user can break down the protein shape in terms of coarse volumes and secondary structure content. This might be easy for some objectives (like “two domains connected by a linker”), but harder for others. Future interfaces might integrate automatic suggestion of ellipsoid configurations given some constraints, or interactive design tools to help users iteratively refine the layout.</p> </li> <li> <p><strong>Physical Realism and Designability</strong>: While ProtComposer improves designability compared to unconstrained generative models by guiding structures, not every generated protein is guaranteed to fold or be biophysically sound. The <strong>flow-matching model</strong> ensures the outputs lie in the space of learned protein-like structures to a large extent, but pushing to very novel shapes can yield odd structures. The authors noted a trade-off where very novel layouts reduced the confidence that sequences would fold into them. This suggests that the model, like others, may sometimes produce “out-of-distribution” structures that a real polypeptide might struggle to realize. In future work, one could incorporate a folding verification step (e.g., running AlphaFold on generated sequences as a filter) or explicitly train the model with a constraint for foldability. Additionally, aspects like side-chain packing, solvation, or disulfide bonds are not explicitly handled by the model (it outputs just backbone and sequence). After generation, structures likely need relaxation or evaluation in a physics-based forcefield to ensure atoms can be placed without clashes. Integrating such considerations (perhaps via a scoring network or energy term during generation) could further improve the physical validity of designs.</p> </li> <li> <p><strong>Scope of Ellipsoid Abstraction</strong>: The ellipsoid approach currently covers spatial layout and secondary structure, but not all aspects of protein design. For example, if one wanted to enforce a specific sequence motif or a binding site geometry (beyond secondary structure), that’s not captured by ellipsoids. We might imagine extending the conditioning to include functional constraints: e.g., “this ellipsoid region should contain a specific pattern or be able to bind X”. That would require augmenting the model with additional conditioning modalities (like specifying certain residues or distances explicitly). ProtComposer’s framework is a step towards multi-modal control, and one can foresee combining it with other conditioning like <strong>motif scaffolding</strong> (as seen in other work) by placing an ellipsoid around a known active site configuration that must be included. Currently, it excels at structural layout control, which is a foundational piece, but further work is needed to incorporate biochemical/functional constraints for full-fledged protein design for function.</p> </li> <li> <p><strong>Computational Cost and Complexity</strong>: Flow matching models, especially with SE(3) equivariance and cross attention, are non-trivial in size and training complexity. ProtComposer’s training involved fine-tuning a large model with powerful compute (likely NVIDIA GPUs, given the authors) and the sampling, while faster than some older methods, is still more involved than a single forward pass (it requires integrating ODEs or simulating a continuous trajectory). The method is thus currently in the realm of research and not plug-and-play like some simpler models. Over time, with optimized implementations or distillation, this could improve. The authors did demonstrate the feasibility by producing many samples for analysis, but if a practitioner wanted to design thousands of proteins with it, they would need access to significant computing resources.</p> </li> </ul> <p>Despite these limitations, the <strong>potential of ProtComposer</strong> is vast. It essentially opens a new design paradigm: <em>“sketch-to-protein”</em> generation. This could revolutionize how protein engineers approach a problem – instead of tweaking sequences and hoping for structures, they can now think in shapes: draw the rough shape of the protein they need for a task, and let the model propose sequences that realize it. As the technology matures, we might see user-friendly tools where one drags ellipsoids in a 3D canvas and the AI outputs a protein model in seconds. This could accelerate the design of novel enzymes, protein cages, therapeutic proteins, and more, by allowing intuitive creativity combined with AI’s knowledge of protein physics.</p> <hr/> <h2 id="conclusion-and-impact-in-structural-biology">Conclusion and Impact in Structural Biology</h2> <p>ProtComposer’s introduction is part of a broader trend of bringing <strong>controllability and compositionality</strong> into generative models for biology. Early protein generation efforts were mostly <em>unconditional</em> – they generated random folds, which while novel, had limited use if you needed something specific. Recently, methods like <strong>diffusion</strong> <d-cite key="trippe2022diffusion"></d-cite> and others allowed partial control (e.g., fixing part of a structure or guiding via a distance map). ProtComposer pushes this further by offering a more global yet intuitive control mechanism. It can be seen as analogous to how <strong>CAD software</strong> is used in mechanical design: one specifies a blueprint, and then the details are filled in. Here, the ellipsoids are the blueprint, and the AI fills in the molecular details.</p> <p>From a geometric deep learning perspective, ProtComposer showcases the power of combining <strong>equivariant networks</strong> with <strong>attention mechanisms</strong> for complex conditional generation. The use of Invariant Cross Attention (ICA) to tie together two sets of geometric objects (ellipsoids and residue frames) is a novel architectural contribution. This idea could be reused in other domains, such as conditioning drug molecule generation on a binding pocket surface (analogous problem in 3D). It also demonstrates the flexibility of the flow-matching framework for handling mixed continuous/discrete data (protein backbone coordinates + sequences) – a relatively cutting-edge approach compared to more common diffusion models. The success of ProtComposer thus contributes to the evolving toolkit of generative modeling: it indicates that <strong>flow models with proper conditioning can achieve results on par with or exceeding diffusion models</strong> on a challenging task, which might inspire more research into flow-based generative models (which have certain advantages in training stability and invertibility).</p> <p>In conclusion, ProtComposer represents a <strong>significant leap</strong> in protein design capability. It empowers researchers to compose proteins in ways previously not possible, by marrying high-level human design intuition (shapes and domains) with low-level sequence generation by AI. Over time, such tools could lead to the creation of proteins with tailor-made architectures for tasks like multienzyme complexes (where you want enzymes held in specific relative orientations), novel vaccines (scaffolding multiple antigens in one protein), or biomaterials (protein cages or fibers of defined shape). The work underscores a key message: introducing the right level of <strong>compositional bias or control</strong> (in this case, ellipsoidal sketches) can dramatically enhance both the <strong>usability</strong> and <strong>performance</strong> of generative models in biology, paving the way for AI-assisted design of biomolecules with unprecedented complexity and precision.</p>]]></content><author><name>Seungjun Yu</name></author><category term="Paper"/><category term="Review"/><category term="2"/><summary type="html"><![CDATA[Hannes Stark, Bowen Jing, Tomas Geffner, Jason Yim, Tommi Jaakkola, Arash Vahdat & Karsten Kreis]]></summary></entry></feed>